{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e22a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97fbd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=20 #we will extract 20 frames from each video \n",
    "img_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b25d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "violence_v_files=os.listdir(r\"C:\\Users\\MUNDI\\Downloads\\archive (5)\\Real Life Violence Dataset\\Violence\")\n",
    "print(len(violence_v_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ef74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=os.path.join(r\"C:\\Users\\MUNDI\\ML\",\"violence_dataset\")\n",
    "Violence_path_list=[]\n",
    "for video in violence_v_files:\n",
    "    path=os.path.join(r\"C:\\Users\\MUNDI\\Downloads\\archive (5)\\Real Life Violence Dataset\\Violence\",video)\n",
    "\n",
    "    vcap=cv2.VideoCapture(path)\n",
    "    frame_count=vcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    \n",
    "    #after much frames we will collect next frame\n",
    "    frame_gap=max(int(frame_count/sequence),1)\n",
    "    \n",
    "    v_path_list=[]\n",
    "    \n",
    "    for frame_counter in range(sequence):\n",
    "       \n",
    "        vcap.set(cv2.CAP_PROP_POS_FRAMES,frame_counter*frame_gap)\n",
    "        \n",
    "        res,frame=vcap.read()\n",
    "        \n",
    "        if not res:\n",
    "            break;\n",
    "        \n",
    "        img=cv2.resize(frame,(img_size,img_size))\n",
    "        \n",
    "        img_name=str(frame_counter)+\"_\"+video[:-4]+\".jpeg\"\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        \n",
    "        cv2.imwrite(img_path,img)\n",
    "        v_path_list.append(img_path)\n",
    "    \n",
    "    vcap.release()\n",
    "    Violence_path_list.append(v_path_list)\n",
    "    \n",
    "with open(\"violence_v_path.pkl\",\"wb\") as f:\n",
    "    pickle.dump(Violence_path_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628549c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"violence_v_path.pkl\",\"rb\") as f:\n",
    "    load_list=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec615cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(load_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c6ec6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "non_violence_v_files=os.listdir(r\"C:\\Users\\MUNDI\\Downloads\\archive (5)\\Real Life Violence Dataset\\NonViolence\")\n",
    "print(len(non_violence_v_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69c4c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path=os.path.join(r\"C:\\Users\\MUNDI\\ML\",\"non_violence_dataset\")\n",
    "non_Violence_path_list=[]\n",
    "for video in non_violence_v_files:\n",
    "    path=os.path.join(r\"C:\\Users\\MUNDI\\Downloads\\archive (5)\\Real Life Violence Dataset\\NonViolence\",video)\n",
    "    \n",
    "    vcap=cv2.VideoCapture(path)\n",
    "    frame_count=vcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    \n",
    "    #after much frames we will collect next frame\n",
    "    frame_gap=max(int(frame_count/sequence),1)\n",
    "    \n",
    "    v_path_list=[]\n",
    "    \n",
    "    for frame_counter in range(sequence):\n",
    "       \n",
    "        vcap.set(cv2.CAP_PROP_POS_FRAMES,frame_counter*frame_gap)\n",
    "        \n",
    "        res,frame=vcap.read()\n",
    "        \n",
    "        if not res:\n",
    "            break;\n",
    "       \n",
    "        img=cv2.resize(frame,(img_size,img_size))\n",
    "        \n",
    "        img_name=str(frame_counter)+\"_\"+video[:-4]+\".jpeg\"\n",
    "        img_path=os.path.join(folder_path,img_name)\n",
    "        \n",
    "        cv2.imwrite(img_path,img)\n",
    "        v_path_list.append(img_path)\n",
    "    \n",
    "    vcap.release()\n",
    "    non_Violence_path_list.append(v_path_list)\n",
    "    \n",
    "with open(\"non_violence_v_path.pkl\",\"wb\") as f:\n",
    "    pickle.dump(non_Violence_path_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adf15bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "with open(\"non_violence_v_path.pkl\",\"rb\") as f:\n",
    "    list=pickle.load(f)\n",
    "print(len(list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
